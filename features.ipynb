{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "import gensim.downloader as api\n",
    "from redditparser import Reddit_Parser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math\n",
    "\n",
    "# Around 4 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The next part should be ignored: is for building the corpus and I did it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_easy = 'data/pan/easy/train'\n",
    "train_dir_medium = 'data/pan/medium/train'\n",
    "train_dir_hard = 'data/pan/hard/train'\n",
    "\n",
    "train_set = [train_dir_easy, train_dir_medium, train_dir_hard]\n",
    "\n",
    "eval_dir_easy = 'data/pan/easy/validation'\n",
    "eval_dir_medium = 'data/pan/medium/validation'\n",
    "eval_dir_hard = 'data/pan/hard/validation'\n",
    "\n",
    "eval_set = [eval_dir_easy, eval_dir_medium, eval_dir_hard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "myparser_easy = Reddit_Parser(train_dir_easy, eval_dir_easy, \"easyparser\")\n",
    "myparser_medium = Reddit_Parser(train_dir_medium, eval_dir_medium, \"mediumparser\")\n",
    "myparser_hard = Reddit_Parser(train_dir_hard, eval_dir_hard, \"hardparser\")\n",
    "\n",
    "myparsers = [myparser_easy, myparser_medium, myparser_hard]\n",
    "\n",
    "for parser in myparsers:\n",
    "    parser.get_data()\n",
    "\n",
    "# Around 4m30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13016\n",
      "19806\n",
      "23092\n",
      "55914\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for parser in myparsers:\n",
    "    listed = parser.train_single_sents['Paragraphs'].tolist()\n",
    "    print(len(listed))\n",
    "    corpus.extend(listed)\n",
    "\n",
    "# Around\n",
    "\n",
    "print(len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55914\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55914\n",
      "How interesting. It'd be good if peace was negotiated. Everyone can save face. Finally, you can still pump Ukraine with finances and arms, and turn it into a second poland and probably stave off war for another decade or two or longer. Basically like Korea.\n",
      "Corpus saved to corpus.txt\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "\n",
    "print(corpus[2])\n",
    "\n",
    "separator = ' [SEP] '\n",
    "corpus_string = separator.join(corpus)\n",
    "\n",
    "with open('corpus.txt', \"w\", encoding='utf-8') as file:\n",
    "    # Write the corpus string to the file\n",
    "    file.write(corpus_string)\n",
    "\n",
    "print(\"Corpus saved to\", 'corpus.txt')\n",
    "\n",
    "# Around\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart from here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16387414\n",
      "Corpus read from corpus.txt\n"
     ]
    }
   ],
   "source": [
    "filename = \"corpus.txt\"\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "    # Read the contents of the file into a variable\n",
    "    corpus = file.read()\n",
    "\n",
    "print(len(corpus))\n",
    "print(\"Corpus read from\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "Relevant stylometric features collected following the survey of Zamir et al. 2024 and Lagutina et al. 2017 \n",
    "\n",
    "\"As a result,\n",
    "simple approaches based on n-grams of characters/words were\n",
    "much more effective than more complex methods based on\n",
    "in-depth study and linguistic analysis of texts\n",
    "However, the text dataset consisted of\n",
    "texts of a specific genre written by non-professional authors.\n",
    "Perhaps this was the reason why the best results were showed\n",
    "by relatively simple methods.\n",
    "\"\n",
    "\n",
    "Takeaway from PAN 2018 \n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "Character Level -> Character ngrams\n",
    "    Stuart et al. -> letter trigrams, letter bigrams, words, functional words, POS bigrams, POS tags, letters, POS -> 95% accuracy\n",
    "    Johnson and Wright -> Email classification\n",
    "Word Level -> Bag-of-words\n",
    "    Use of functional words\n",
    "        Boukhaled and Ganascia -> SVM classification -> 95%\n",
    "    Statistics \n",
    "        Jamak et al.\n",
    "    Ngrams\n",
    "        Brocardo et al. -> 14% EER\n",
    "Sentence Level -> Syntactic features\n",
    "    Adjacency networks \n",
    "        Stanisz et al. -> 85%-90%\n",
    "    Syntax and lexical words\n",
    "        Sundararajan and Woodard -> A syntactic language model was obtained by constructing the probabilistic context-free grammar for each author using the constituency parse trees of sentences in their training posts. The experiments with Guardian articles showed that the method achieved 67– 70% or less of F-measure and accuracy. The authors note that authorship attribution approaches in literature focus mostly on single-domain attribution where content and style are highly entangled. Further analysis shows that syntax may be useful with cross-genre attribution while cross-topic attribution and single-domain attribution may benefit from both syntax and lexical information.\n",
    "    GREAT study about Cristani et al. -> lexical, syntactical, content-specific, idiosincratic - 89% \n",
    "    Gomez-Adorno et al. [24] -> textual features from synctactic graphs -> 83%\n",
    "    \n",
    "\n",
    "Sapkota et al. [23] identified subgroups of\n",
    "character n-grams that corresponded to linguistic aspects commonly\n",
    "claimed to be covered by these features: morpho-syntax,\n",
    "thematic content, and style.\n",
    "Sourussi et al. on emails\n",
    "\n",
    "...\n",
    "Discourse features -> Ferracane et al. -> 99%!\n",
    "Random forest for  feature vector of text -> Llorens and Delany -> 80-90%\n",
    "Potha and Stamatatos -> Latent semantic indexing 80%\n",
    "\n",
    "We are working with a CLOSED-SET ATTRIBUTION\n",
    "\n",
    "-----\n",
    "\n",
    "Studies specifically for Authorship verification\n",
    "Studies specifically for Style change detection (par. 5)\n",
    "\n",
    "PAN 2018: “represented as a consecutive order\n",
    "of parse tree features, sentence-level: stop words, most/least\n",
    "frequent words or word pairs, and punctuation frequencies,\n",
    "statistical text features including number of sentences, text\n",
    "th, and frequencies of unique words, punctuations, or\n",
    "letters, character n-grams, word 1-6-grams” [19]. The best\n",
    "result of 89.3% accuracy was obtained using several different\n",
    "groups of features: character-based, word-based, and sentencebased\n",
    "ones; and popular machine learning classifiers: SVM,\n",
    "Random forest, TF-IDF-based gradient boosting model, and\n",
    "logistic regression meta-classifier.\"\n",
    "\n",
    "'''\n",
    "import math\n",
    "\n",
    "class StylometryExtractor():\n",
    "    def __init__(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.embedding_vocab = api.load(\"glove-wiki-gigaword-300\")\n",
    "        self.top_100_common_words = None\n",
    "        self.top_10_common_adjectives = None\n",
    "        self.top_10_common_conjunctions = None\n",
    "        self.top_10_common_interrogatives = None\n",
    "        self.top_10_common_nouns = None\n",
    "        self.top_10_common_verbs = None\n",
    "        self.top_30_common_three_grams = None\n",
    "        self.top_30_common_five_grams = None\n",
    "        self.top_100_common_oov_embeddings = None\n",
    "        self.top_100_common_tri_grams = None\n",
    "        self.top_100_common_five_grams = None\n",
    "    \n",
    "\n",
    "    def is_consonant(self, char):\n",
    "        consonant_pattern = re.compile(r'[bcdfghjklmnpqrstvwxyz]', re.IGNORECASE)\n",
    "        return bool(consonant_pattern.match(char))\n",
    "\n",
    "    def is_vowel(self, char):\n",
    "        vowel_pattern = re.compile(r'[aeiou]', re.IGNORECASE)\n",
    "        return bool(vowel_pattern.match(char))\n",
    "    \n",
    "    def calculate_avg_tfidf(self, corpus, paragraph):\n",
    "    \n",
    "        df = {}\n",
    "        corpus = corpus.lower()\n",
    "        n = len(corpus)\n",
    "        for doc in corpus:\n",
    "            unique_tokens = set(doc)\n",
    "            for token in unique_tokens:\n",
    "                df[token] = df.get(token, 0) + 1\n",
    "        idf = {}\n",
    "        for token, freq in df.items():\n",
    "            idf[token] = math.log(n / freq)\n",
    "        \n",
    "        tokens = paragraph.split()  # Split by space, you can use more advanced tokenization techniques here\n",
    "    \n",
    "        # Count term frequency\n",
    "        tf = {}\n",
    "        for token in tokens:\n",
    "            tf[token] = tf.get(token, 0) + 1\n",
    "        \n",
    "        # Normalize term frequency\n",
    "        total_tokens = len(tokens)\n",
    "        for token, freq in tf.items():\n",
    "            tf[token] = freq / total_tokens\n",
    "\n",
    "        tfidf = {}\n",
    "        for token, tf_score in tf.items():\n",
    "            idf_score = idf.get(token, 0)  # If token not found in IDF scores, assume IDF = 0\n",
    "            tfidf[token] = tf_score * idf_score\n",
    "        \n",
    "        return sum(tfidf.values()) / len(tokens)\n",
    "\n",
    "    def preprocess_corpus(self):\n",
    "        \n",
    "        corpus = self.corpus\n",
    "        print(\"Saved corpus variable\")\n",
    "\n",
    "        # Preprocessing step\n",
    "\n",
    "        corpus_lower = corpus.lower()\n",
    "        print(\"lowered\")\n",
    "        tokens = word_tokenize(corpus_lower)\n",
    "        print(\"tokenized\")\n",
    "        tagged_tokens = pos_tag(tokens)\n",
    "        print(\"pos_tagged\")\n",
    "        freq_dist = FreqDist(tagged_tokens)\n",
    "        freq_dist_no_tag = FreqDist(tokens)\n",
    "        print(\"general freq dist done\")\n",
    "        vocabulary = freq_dist_no_tag.keys()\n",
    "        print(\"vocabulary initialized\")\n",
    "        adjectives = [word for word, tag in tagged_tokens if tag.startswith('JJ')]\n",
    "        print(\"adjectives done\")\n",
    "        conjunctions = [word for word, tag in tagged_tokens if tag.startswith('CC')]\n",
    "        print(\"conjunctions done\")\n",
    "        interrogatives = [word for word, tag in tagged_tokens if tag in ['WP', 'WRB']]\n",
    "        print(\"interrogatives done\")\n",
    "        nouns = [word for word, tag in tagged_tokens if tag in ['NN', 'NNS', 'NNPS', 'NNP']]\n",
    "        print(\"nouns done\")\n",
    "        verbs = [word for word, tag in tagged_tokens if tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]\n",
    "        print(\"verbs done\")\n",
    "        three_grams = list(ngrams(tokens, 3))\n",
    "        print(\"3grams done\")\n",
    "        five_grams = list(ngrams(tokens, 5))\n",
    "        print(\"5grams done\")\n",
    "        oov_embeddings = [word for word in vocabulary if word not in self.embedding_vocab]\n",
    "        print(\"oov done\")\n",
    "\n",
    "        ''''''\n",
    "        freq_dist_adjectives = FreqDist(adjectives)\n",
    "        print(\"collected freq_dists for adjectives\")\n",
    "        freq_dist_conjunctions = FreqDist(conjunctions)\n",
    "        print(\"collected freq_dists for conjunctions\")\n",
    "        freq_dist_interrogatives = FreqDist(interrogatives)\n",
    "        print(\"collected freq_dists for interrogatives\")\n",
    "        freq_dist_nouns = FreqDist(nouns)\n",
    "        print(\"collected freq_dists for nouns\")\n",
    "        freq_dist_verbs = FreqDist(verbs)\n",
    "        print(\"collected freq_dists for verbs\")\n",
    "        freq_dist_oov_embeddings = FreqDist(oov_embeddings)\n",
    "        print(\"collected freq_dists for oovs\")\n",
    "        freq_dist_three_grams = FreqDist(three_grams)\n",
    "        print(\"collected freq_dists for 3grams\")\n",
    "        freq_dist_five_grams = FreqDist(five_grams)\n",
    "        print(\"collected freq_dists for 5grams\")\n",
    "\n",
    "        ''''''\n",
    "        print(\"starting collecting most commons\")\n",
    "        self.top_100_common_words = [x[0] for x in freq_dist.most_common(100)]\n",
    "        self.top_10_common_adjectives = [x[0] for x in freq_dist_adjectives.most_common(10)]\n",
    "        self.top_10_common_conjunctions = [x[0] for x in freq_dist_conjunctions.most_common(10)]\n",
    "        self.top_10_common_interrogatives = [x[0] for x in freq_dist_interrogatives.most_common(10)]\n",
    "        self.top_10_common_nouns = [x[0] for x in freq_dist_nouns.most_common(10)]\n",
    "        self.top_10_common_verbs = [x[0] for x in freq_dist_verbs.most_common(10)]\n",
    "        self.top_30_common_three_grams = [x[0] for x in freq_dist_three_grams.most_common(30)]\n",
    "        self.top_30_common_five_grams = [x[0] for x in freq_dist_five_grams.most_common(30)]\n",
    "        self.top_100_common_oov_embeddings = [x[0] for x in freq_dist_oov_embeddings.most_common(100)]\n",
    "        self.top_100_common_tri_grams = [x[0] for x in freq_dist_five_grams.most_common(100)]\n",
    "        self.top_100_common_five_grams = [x[0] for x in freq_dist_three_grams.most_common(100)]\n",
    "\n",
    "        # Update init to save info about the corpus\n",
    "\n",
    "    def get_values(self, value):\n",
    "\n",
    "        if value == 'words':\n",
    "            return self.top_100_common_words\n",
    "        elif value == 'adjectives':\n",
    "            return self.top_10_common_adjectives\n",
    "        elif value == 'conjunctions':\n",
    "            return self.top_10_common_conjunctions\n",
    "        elif value == 'interrogatives':\n",
    "            return self.top_10_common_interrogatives\n",
    "        elif value == 'nouns':\n",
    "            return self.top_10_common_nouns\n",
    "        elif value == 'verbs':\n",
    "            return self.top_10_common_verbs\n",
    "        elif value == 'three_grams':\n",
    "            return self.top_30_common_three_grams\n",
    "        elif value == 'five_grams':\n",
    "            return self.top_30_common_five_grams\n",
    "        elif value == 'oov':\n",
    "            return self.top_100_common_oov_embeddings\n",
    "        else:\n",
    "            ValueError('Set value')\n",
    "\n",
    "    def stylometry_extractor(self, paragraph, character_level=True, word_level=True, sentence_level=True):\n",
    "\n",
    "        # Some init\n",
    "        F = np.array([])\n",
    "        F = F.reshape(-1)\n",
    "        F_chars = np.zeros(104)\n",
    "        F_words = np.zeros(60)\n",
    "        F_sents = np.zeros(220)\n",
    "\n",
    "        epsilon = 1e-10\n",
    "\n",
    "        # Preprocess paragraph\n",
    "        sentences = sent_tokenize(paragraph)\n",
    "        tokens = word_tokenize(paragraph)\n",
    "        paragraph_lower = paragraph.lower()\n",
    "        tokens_lower = word_tokenize(paragraph_lower)\n",
    "        pos_tagged_tokens = pos_tag(tokens)\n",
    "        parts_of_speech = [x[1] for x in pos_tagged_tokens]\n",
    "\n",
    "        # Set regexs\n",
    "\n",
    "        punctuation = r'[\\W_]+'\n",
    "        other_things = r'[^a-zA-Z0-9\\s\\\\p{P}]'\n",
    "        words = len(tokens)\n",
    "\n",
    "        # Character level\n",
    "\n",
    "        chars_list = list(paragraph_lower)\n",
    "        chars = len(list(paragraph_lower))\n",
    "        alphas = len([char for char in chars_list if char.isalpha()])\n",
    "        uppers = len([char for char in list(paragraph) if char.isupper()])\n",
    "        digits = len([char for char in chars_list if char.isdigit()])\n",
    "        whitespaces = len([char for char in chars_list if char == ' '])\n",
    "        vowels = len([char for char in paragraph if char.lower() in 'aeiou'])\n",
    "        char_two_grams = len([''.join(gram) for gram in list(ngrams(chars_list, 2))])\n",
    "        consonant_vowel_twograms = len([''.join(gram) for gram in ngrams(chars_list, 2) if re.match(r'[bcdfghjklmnpqrstvwxyz][aeiou]', ''.join(gram))])\n",
    "        vowel_consonant_twograms = len([''.join(gram) for gram in ngrams(chars_list, 2) if re.match(r'[aeiou][bcdfghjklmnpqrstvwxyz]', ''.join(gram))])\n",
    "        consonant_consonant_twograms = len([''.join(gram) for gram in ngrams(chars_list, 2) if re.match(r'[bcdfghjklmnpqrstvwxyz][bcdfghjklmnpqrstvwxyz]', ''.join(gram))])\n",
    "        vowel_vowel_twograms = len([''.join(gram) for gram in ngrams(chars_list, 2) if re.match(r'[aeiou][aeiou]', ''.join(gram))])\n",
    "\n",
    "        # Word level\n",
    "\n",
    "        word_three_grams = list(ngrams(tokens, 3))\n",
    "        word_five_grams = list(ngrams(tokens, 5))\n",
    "\n",
    "        # Sentence level \n",
    "\n",
    "        puncts = len(re.findall(punctuation, paragraph))\n",
    "        others = len(re.findall(other_things, paragraph)) \n",
    "        pp = len([pos_tag for pos_tag in parts_of_speech if pos_tag.startswith('PRP')])\n",
    "        adj = len([pos_tag for pos_tag in parts_of_speech if pos_tag.startswith('JJ')])\n",
    "        conj = len([pos_tag for pos_tag in parts_of_speech if pos_tag.startswith('CC')])\n",
    "        aux = len([pos_tag for pos_tag in parts_of_speech if pos_tag.startswith('MD')])\n",
    "        interr = len([pos_tag for pos_tag in parts_of_speech if pos_tag in ['WP', 'WRB']])\n",
    "        nouns = len([pos_tag for pos_tag in parts_of_speech if pos_tag in ['NN', 'NNS', 'NNPS', 'NNP']])\n",
    "        verbs = len([pos_tag for pos_tag in parts_of_speech if pos_tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']])\n",
    "        dets = len([pos_tag for pos_tag in parts_of_speech if pos_tag.startswith('DT')])\n",
    "        articles = len([token for token in tokens_lower if token in [\"a\", \"an\", \"the\"]])\n",
    "        sents = len(nltk.sent_tokenize(paragraph)) \n",
    "        top100_common_words = [word for word, _ in self.top_100_common_words]\n",
    "\n",
    "        # Build feature vector\n",
    "\n",
    "        freq_cons_0 = [\"t\", \"n\", \"s\", \"r\", \"h\"]\n",
    "        count_freq_cons_0 = sum(paragraph.count(char) for char in freq_cons_0)\n",
    "        freq_cons_1 = [\"l\", \"d\", \"c\", \"p\", \"f\"]\n",
    "        count_freq_cons_1 = sum(paragraph.count(char) for char in freq_cons_1)\n",
    "        freq_cons_2 = [\"m\", \"w\", \"y\", \"b\", \"g\"]\n",
    "        count_freq_cons_2 = sum(paragraph.count(char) for char in freq_cons_2)\n",
    "        freq_cons_3 = [\"j\", \"k\", \"q\", \"v\", \"x\", \"z\"]\n",
    "        count_freq_cons_3 = sum(paragraph.count(char) for char in freq_cons_3)\n",
    "        to_be_verbs = [\" am \", \" are \", \" be \", \" been \", \" being \", \" is \", \" was \", \" were \"]\n",
    "        count_to_be_verbs = sum(paragraph.count(word) for word in to_be_verbs)\n",
    "\n",
    "        def count_missed_uppercase(tokens):\n",
    "            missed_uppercase_count = 0\n",
    "\n",
    "            for i in range(len(tokens) - 1):\n",
    "                if tokens[i] == '.' and tokens[i+1][0].islower():\n",
    "                    missed_uppercase_count += 1\n",
    "\n",
    "            return missed_uppercase_count\n",
    "        \n",
    "        def count_missing_periods(sentences):\n",
    "            missing_period_count = 0\n",
    "\n",
    "            for sentence in sentences:\n",
    "                if sentence[-1] != '.':\n",
    "                    missing_period_count += 1\n",
    "\n",
    "            return missing_period_count\n",
    "\n",
    "        if character_level:\n",
    "            F_chars = [\n",
    "                alphas/(chars + epsilon),\n",
    "                uppers/(chars + epsilon),\n",
    "                digits/(chars + epsilon),\n",
    "                whitespaces/(chars + epsilon),\n",
    "                vowels/(chars + epsilon),\n",
    "                paragraph_lower.count(\"a\")/(vowels + epsilon),\n",
    "                paragraph_lower.count(\"e\")/(vowels + epsilon),\n",
    "                paragraph_lower.count(\"i\")/(vowels + epsilon),\n",
    "                paragraph_lower.count(\"o\")/(vowels + epsilon),\n",
    "                paragraph_lower.count(\"u\")/(vowels + epsilon),\n",
    "                paragraph_lower.count(\"a\")/(chars + epsilon),\n",
    "                count_freq_cons_0/(alphas + epsilon),\n",
    "                count_freq_cons_1/(alphas + epsilon),\n",
    "                count_freq_cons_2/(alphas + epsilon),\n",
    "                count_freq_cons_3/(alphas + epsilon),\n",
    "                paragraph_lower.count(\"t\")/(count_freq_cons_0 + epsilon),\n",
    "                paragraph_lower.count(\"n\")/(count_freq_cons_0 + epsilon),\n",
    "                paragraph_lower.count(\"s\")/(count_freq_cons_0 + epsilon),\n",
    "                paragraph_lower.count(\"r\")/(count_freq_cons_0 + epsilon),\n",
    "                paragraph_lower.count(\"h\")/(count_freq_cons_0 + epsilon),\n",
    "                paragraph_lower.count(\"l\")/(count_freq_cons_1 + epsilon),\n",
    "                paragraph_lower.count(\"d\")/(count_freq_cons_1 + epsilon),\n",
    "                paragraph_lower.count(\"c\")/(count_freq_cons_1 + epsilon),\n",
    "                paragraph_lower.count(\"p\")/(count_freq_cons_1 + epsilon),\n",
    "                paragraph_lower.count(\"f\")/(count_freq_cons_1 + epsilon),\n",
    "                paragraph_lower.count(\"m\")/(count_freq_cons_2 + epsilon),\n",
    "                paragraph_lower.count(\"w\")/(count_freq_cons_2 + epsilon),\n",
    "                paragraph_lower.count(\"y\")/(count_freq_cons_2 + epsilon),\n",
    "                paragraph_lower.count(\"b\")/(count_freq_cons_2 + epsilon),\n",
    "                paragraph_lower.count(\"g\")/(count_freq_cons_2 + epsilon),\n",
    "                paragraph_lower.count(\"j\")/(count_freq_cons_3 + epsilon),\n",
    "                paragraph_lower.count(\"k\")/(count_freq_cons_3 + epsilon),\n",
    "                paragraph_lower.count(\"q\")/(count_freq_cons_3 + epsilon),\n",
    "                paragraph_lower.count(\"v\")/(count_freq_cons_3 + epsilon),\n",
    "                paragraph_lower.count(\"x\")/(count_freq_cons_3 + epsilon),\n",
    "                paragraph_lower.count(\"z\")/(count_freq_cons_3 + epsilon),\n",
    "                consonant_consonant_twograms/(char_two_grams + epsilon),\n",
    "                vowel_consonant_twograms/(char_two_grams + epsilon),\n",
    "                consonant_vowel_twograms/(char_two_grams + epsilon),\n",
    "                vowel_vowel_twograms/(char_two_grams + epsilon),\n",
    "                paragraph_lower.count(\"st\")/(consonant_consonant_twograms + epsilon),\n",
    "                paragraph_lower.count(\"nd\")/(consonant_consonant_twograms + epsilon),\n",
    "                paragraph_lower.count(\"th\")/(consonant_consonant_twograms + epsilon),\n",
    "                paragraph_lower.count(\"an\")/(vowel_consonant_twograms + epsilon),\n",
    "                paragraph_lower.count(\"in\")/(vowel_consonant_twograms + epsilon),\n",
    "                paragraph_lower.count(\"er\")/(vowel_consonant_twograms + epsilon),\n",
    "                paragraph_lower.count(\"es\")/(vowel_consonant_twograms + epsilon),\n",
    "                paragraph_lower.count(\"on\")/(vowel_consonant_twograms + epsilon),\n",
    "                paragraph_lower.count(\"at\")/(vowel_consonant_twograms + epsilon),\n",
    "                paragraph_lower.count(\"en\")/(vowel_consonant_twograms + epsilon),\n",
    "                paragraph_lower.count(\"or\")/(vowel_consonant_twograms + epsilon),\n",
    "                paragraph_lower.count(\"he\")/(consonant_vowel_twograms + epsilon),\n",
    "                paragraph_lower.count(\"re\")/(consonant_vowel_twograms + epsilon),\n",
    "                paragraph_lower.count(\"ti\")/(consonant_vowel_twograms + epsilon),\n",
    "                paragraph_lower.count(\"ea\")/(vowel_vowel_twograms + epsilon),\n",
    "                sum(1 for i in range(len(paragraph_lower) - 1) if paragraph_lower[i].isalpha() and paragraph_lower[i] == paragraph_lower[i + 1])/(char_two_grams + epsilon),\n",
    "                chars/100,\n",
    "                alphas/100,\n",
    "                uppers/100,\n",
    "                paragraph_lower.count(\"a\"),\n",
    "                paragraph_lower.count(\"b\"),\n",
    "                paragraph_lower.count(\"c\"),\n",
    "                paragraph_lower.count(\"d\"),\n",
    "                paragraph_lower.count(\"e\"),\n",
    "                paragraph_lower.count(\"f\"),\n",
    "                paragraph_lower.count(\"g\"),\n",
    "                paragraph_lower.count(\"h\"),\n",
    "                paragraph_lower.count(\"i\"),\n",
    "                paragraph_lower.count(\"j\"),\n",
    "                paragraph_lower.count(\"k\"),\n",
    "                paragraph_lower.count(\"l\"),\n",
    "                paragraph_lower.count(\"m\"),\n",
    "                paragraph_lower.count(\"n\"),\n",
    "                paragraph_lower.count(\"o\"),\n",
    "                paragraph_lower.count(\"p\"),\n",
    "                paragraph_lower.count(\"q\"),\n",
    "                paragraph_lower.count(\"r\"),\n",
    "                paragraph_lower.count(\"s\"),\n",
    "                paragraph_lower.count(\"t\"),\n",
    "                paragraph_lower.count(\"u\"),\n",
    "                paragraph_lower.count(\"v\"),\n",
    "                paragraph_lower.count(\"w\"),\n",
    "                paragraph_lower.count(\"x\"),\n",
    "                paragraph_lower.count(\"y\"),\n",
    "                paragraph_lower.count(\"z\"),\n",
    "                paragraph_lower.count(\"~\"),\n",
    "                paragraph_lower.count(\"@\"),\n",
    "                paragraph_lower.count(\"#\"),\n",
    "                paragraph_lower.count(\"$\"),\n",
    "                paragraph_lower.count(\"%\"),\n",
    "                paragraph_lower.count(\"^\"),\n",
    "                paragraph_lower.count(\"&\"),\n",
    "                paragraph_lower.count(\"*\"),\n",
    "                paragraph_lower.count(\"-\"),\n",
    "                paragraph_lower.count(\"=\"),\n",
    "                paragraph_lower.count(\"+\"),\n",
    "                paragraph_lower.count(\">\"),\n",
    "                paragraph_lower.count(\"<\"),\n",
    "                paragraph_lower.count(\"[\"),\n",
    "                paragraph_lower.count(\"]\"),\n",
    "                paragraph_lower.count(\"{\"),\n",
    "                paragraph_lower.count(\"}\"),\n",
    "                paragraph_lower.count(\"/\"),\n",
    "                paragraph_lower.count(\"\\\\\"),\n",
    "                paragraph_lower.count(\"|\"),\n",
    "            ]\n",
    "            F = np.concatenate((F, F_chars))\n",
    "\n",
    "        if word_level:\n",
    "            F_words = [\n",
    "                len([token for token in tokens if len(token) == 1])/(words + epsilon),\n",
    "                len([token for token in tokens if len(token) == 2])/(words + epsilon),\n",
    "                len([token for token in tokens if len(token) == 3])/(words + epsilon),\n",
    "                len([token for token in tokens if len(token) == 4])/(words + epsilon),\n",
    "                len([token for token in tokens if len(token) == 5])/(words + epsilon),\n",
    "                len([token for token in tokens if len(token) == 6])/(words + epsilon),\n",
    "                len([token for token in tokens if len(token) == 7])/(words + epsilon),\n",
    "                len([token for token in tokens if len(token) >= 8])/(words + epsilon),\n",
    "                len([token for token in tokens if len(token) <= 3])/(words + epsilon),\n",
    "                chars/(words + epsilon),\n",
    "                len(set(tokens))/(words + epsilon),\n",
    "                words, \n",
    "                len([token for token in tokens if len(token) <= 3]),\n",
    "                len([token for token in tokens if len(token) == 1]),\n",
    "                len([token for token in tokens if len(token) == 2]),\n",
    "                len([token for token in tokens if len(token) == 3]),\n",
    "                len([token for token in tokens if len(token) == 4]),\n",
    "                len([token for token in tokens if len(token) == 5]),\n",
    "                len([token for token in tokens if len(token) == 6]),\n",
    "                len([token for token in tokens if len(token) == 7]),\n",
    "                len([token for token in tokens if len(token) == 8]),\n",
    "                len([token for token in tokens if len(token) == 9]),\n",
    "                len([token for token in tokens if len(token) == 10]),\n",
    "                len([token for token in tokens if len(token) == 11]),\n",
    "                len([token for token in tokens if len(token) == 12]),\n",
    "                len([token for token in tokens if len(token) >= 13]),\n",
    "                paragraph_lower.count(\":)\"),\n",
    "                paragraph_lower.count(\":(\"),\n",
    "                paragraph_lower.count(r'\\b(lol)\\b'),\n",
    "                paragraph_lower.count(\";)\"),\n",
    "                paragraph_lower.count(\"...\"),\n",
    "                paragraph_lower.count(\"cmv\"),\n",
    "                paragraph_lower.count(\"eli5\"),\n",
    "                paragraph_lower.count(\"iirc\"),\n",
    "                paragraph_lower.count(\"imo\"),\n",
    "                paragraph_lower.count(\"imho\"),\n",
    "                paragraph_lower.count(\"irl\"),\n",
    "                paragraph_lower.count(\"mrw\"),\n",
    "                paragraph_lower.count(\"mfw\"),\n",
    "                paragraph_lower.count(\"nsfl\"),\n",
    "                paragraph_lower.count(\"nsfw\"),\n",
    "                paragraph_lower.count(r'\\b(op)\\b'),\n",
    "                paragraph_lower.count(r'\\b(oc)\\b'),\n",
    "                paragraph_lower.count(\"psa\"),\n",
    "                paragraph_lower.count(\"tldr\"),\n",
    "                paragraph_lower.count(\"tl;dr\"),\n",
    "                paragraph_lower.count(r'\\b(til)\\b'),\n",
    "                paragraph_lower.count(\"wip\"),\n",
    "                paragraph_lower.count(\"ysk\"),\n",
    "                paragraph_lower.count(\"aka\"),\n",
    "                paragraph_lower.count(\"goat\"),\n",
    "                paragraph_lower.count(r'\\b(ffs)\\b'),\n",
    "                paragraph_lower.count(\"fyi\"),\n",
    "                paragraph_lower.count(\"tbh\"),\n",
    "                paragraph_lower.count(\"ikr\"),\n",
    "                count_missed_uppercase(tokens),\n",
    "                count_missing_periods(sentences),\n",
    "                # self.calculate_avg_tfidf(corpus, paragraph_lower),\n",
    "                sum(1 for token in tokens if token in self.top_100_common_oov_embeddings)/(words + epsilon),\n",
    "                sum(1 for trigram in word_three_grams if trigram in self.top_100_common_tri_grams)/(len(word_three_grams) + epsilon),\n",
    "                sum(1 for fivegram in word_five_grams if fivegram in self.top_100_common_five_grams)/(len(word_five_grams) + epsilon),\n",
    "            ]\n",
    "            F = np.concatenate((F, F_words))\n",
    "\n",
    "        if sentence_level:\n",
    "\n",
    "            F_sents = [\n",
    "                sents,\n",
    "                puncts/chars,\n",
    "                paragraph_lower.count('.')/(puncts + epsilon),\n",
    "                paragraph_lower.count(',')/(puncts + epsilon),\n",
    "                paragraph_lower.count('?')/(puncts + epsilon),\n",
    "                paragraph_lower.count('!')/(puncts + epsilon),\n",
    "                paragraph_lower.count(';')/(puncts + epsilon),\n",
    "                paragraph_lower.count(':')/(puncts + epsilon),\n",
    "                paragraph_lower.count('\\'')/(puncts + epsilon),\n",
    "                paragraph_lower.count('\\\"')/(puncts + epsilon),\n",
    "                others/(chars + epsilon),\n",
    "                digits/(others + epsilon),\n",
    "                conj/(words + epsilon),\n",
    "                interr/(words + epsilon),\n",
    "                pp/(words + epsilon),\n",
    "                nouns/(words + epsilon),\n",
    "                verbs/(words + epsilon),\n",
    "                adj/(words + epsilon),\n",
    "                articles/(words + epsilon),\n",
    "                articles/(adj + epsilon),\n",
    "                dets/(words + epsilon),\n",
    "                aux/(words + epsilon),\n",
    "                aux/(verbs + epsilon),\n",
    "                chars/(sents + epsilon),\n",
    "                words/(sents + epsilon),\n",
    "                paragraph_lower.count(\"can\")/(aux + epsilon),\n",
    "                paragraph_lower.count(\"did\")/(aux + epsilon),\n",
    "                paragraph_lower.count(r'\\b(do)\\b')/(aux + epsilon),\n",
    "                paragraph_lower.count(\"does\")/(aux + epsilon),\n",
    "                paragraph_lower.count(r'\\b(had)\\b')/(aux + epsilon),\n",
    "                paragraph_lower.count(r'\\b(has)\\b')/(aux + epsilon),\n",
    "                paragraph_lower.count(\"have\")/(aux + epsilon),\n",
    "                paragraph_lower.count(\"could\")/(aux + epsilon),\n",
    "                paragraph_lower.count(\"should\")/(aux + epsilon),\n",
    "                paragraph_lower.count(\"would\")/(aux + epsilon),\n",
    "                paragraph_lower.count(r'\\b(will)\\b')/(aux + epsilon),\n",
    "                count_to_be_verbs/(words + epsilon),\n",
    "                count_to_be_verbs/(verbs + epsilon),\n",
    "                paragraph_lower.count(r'\\b(am)\\b')/(count_to_be_verbs + epsilon),\n",
    "                paragraph_lower.count(r'\\b(are)\\b')/(count_to_be_verbs + epsilon),\n",
    "                paragraph_lower.count(r'\\b(be)\\b')/(count_to_be_verbs + epsilon),\n",
    "                paragraph_lower.count(r'\\b(is)\\b')/(count_to_be_verbs + epsilon),\n",
    "                paragraph_lower.count(r'\\b(was)\\b')/(count_to_be_verbs + epsilon),\n",
    "                paragraph_lower.count(r'\\b(were)\\b')/(count_to_be_verbs + epsilon),\n",
    "                paragraph_lower.count(r'\\b(the)\\b')/(articles + epsilon),\n",
    "                paragraph_lower.count(r'\\b(a)\\b')/(articles + epsilon),\n",
    "                paragraph_lower.count(r'\\b(an)\\b')/(articles + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[0])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[1])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[2])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[3])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[4])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[5])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[6])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[7])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[8])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[9])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[10])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[11])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[12])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[13])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[14])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[15])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[16])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[17])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[18])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[19])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[20])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[21])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[22])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[23])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[24])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[25])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[26])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[27])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[28])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[29])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[30])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[31])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[32])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[33])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[34])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[35])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[36])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[37])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[38])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[39])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[40])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[41])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[42])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[43])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[44])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[45])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[46])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[47])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[48])/(words + epsilon),\n",
    "                paragraph_lower.count(top100_common_words[49])/(words + epsilon),\n",
    "                paragraph_lower.count(r'\\b(a)\\b'),\n",
    "                paragraph_lower.count('about'),\n",
    "                paragraph_lower.count('above'),\n",
    "                paragraph_lower.count('after'),\n",
    "                paragraph_lower.count(r'\\b(all)\\b'),\n",
    "                paragraph_lower.count('although'),\n",
    "                paragraph_lower.count(r'\\b(am)\\b'),\n",
    "                paragraph_lower.count('among'),\n",
    "                paragraph_lower.count(r'\\b(an)\\b'),\n",
    "                paragraph_lower.count(r'\\b(and)\\b'),\n",
    "                paragraph_lower.count('another'),\n",
    "                paragraph_lower.count(r'\\b(any)\\b'),\n",
    "                paragraph_lower.count('anybody'),\n",
    "                paragraph_lower.count('anyone'),\n",
    "                paragraph_lower.count('anything'),\n",
    "                paragraph_lower.count(r'\\b(are)\\b'),\n",
    "                paragraph_lower.count('around'),\n",
    "                paragraph_lower.count(r'\\b(as)\\b'),\n",
    "                paragraph_lower.count(r'\\b(at)\\b'),\n",
    "                paragraph_lower.count(r'\\b(be)\\b'),\n",
    "                paragraph_lower.count('because'),\n",
    "                paragraph_lower.count('before'),\n",
    "                paragraph_lower.count('behind'),\n",
    "                paragraph_lower.count('below'),\n",
    "                paragraph_lower.count('beside'),\n",
    "                paragraph_lower.count('between'),\n",
    "                paragraph_lower.count('both'),\n",
    "                paragraph_lower.count(r'\\b(but)\\b'),\n",
    "                paragraph_lower.count(r'\\b(by)\\b'),\n",
    "                paragraph_lower.count(r'\\b(can)\\b'),\n",
    "                paragraph_lower.count(r'\\b(do)\\b'),\n",
    "                paragraph_lower.count('down'),\n",
    "                paragraph_lower.count('each'),\n",
    "                paragraph_lower.count('either'),\n",
    "                paragraph_lower.count('enough'),\n",
    "                paragraph_lower.count('every'),\n",
    "                paragraph_lower.count('everybody'),\n",
    "                paragraph_lower.count('everyone'),\n",
    "                paragraph_lower.count('everything'),\n",
    "                paragraph_lower.count('few'),\n",
    "                paragraph_lower.count('following'),\n",
    "                paragraph_lower.count(r'\\b(for)\\b'),\n",
    "                paragraph_lower.count('from'),\n",
    "                paragraph_lower.count('have'),\n",
    "                paragraph_lower.count(r'\\b(he)\\b'),\n",
    "                paragraph_lower.count(r'\\b(her)\\b'),\n",
    "                paragraph_lower.count(r'\\b(him)\\b'),\n",
    "                paragraph_lower.count(r'\\b(i)\\b'),\n",
    "                paragraph_lower.count(r'\\b(if)\\b'),\n",
    "                paragraph_lower.count(r'\\b(in)\\b'),\n",
    "                paragraph_lower.count('including'),\n",
    "                paragraph_lower.count('inside'),\n",
    "                paragraph_lower.count(r'\\b(into)\\b'),\n",
    "                paragraph_lower.count(r'\\b(is)\\b'),\n",
    "                paragraph_lower.count(r'\\b(it)\\b'),\n",
    "                paragraph_lower.count(r'\\b(its)\\b'),\n",
    "                paragraph_lower.count('latter'),\n",
    "                paragraph_lower.count(r'\\b(less)\\b'),\n",
    "                paragraph_lower.count(r'\\b(like)\\b'),\n",
    "                paragraph_lower.count('little'),\n",
    "                paragraph_lower.count(r'\\b(lots)\\b'),\n",
    "                paragraph_lower.count('many'),\n",
    "                paragraph_lower.count(r'\\b(me)\\b'),\n",
    "                paragraph_lower.count(r'\\b(more)\\b'),\n",
    "                paragraph_lower.count(r'\\b(most)\\b'),\n",
    "                paragraph_lower.count('much'),\n",
    "                paragraph_lower.count(r'\\b(my)\\b'),\n",
    "                paragraph_lower.count('need'),\n",
    "                paragraph_lower.count('neither'),\n",
    "                paragraph_lower.count(r'\\b(no)\\b'),\n",
    "                paragraph_lower.count('nobody'),\n",
    "                paragraph_lower.count(r'\\b(none)\\b'),\n",
    "                paragraph_lower.count(r'\\b(nor)\\b'),\n",
    "                paragraph_lower.count('nothing'),\n",
    "                paragraph_lower.count(r'\\b(of)\\b'),\n",
    "                paragraph_lower.count(r'\\b(off)\\b'),\n",
    "                paragraph_lower.count(r'\\b(on)\\b'),\n",
    "                paragraph_lower.count('once'),\n",
    "                paragraph_lower.count(r'\\b(one)\\b'),\n",
    "                paragraph_lower.count('onto'),\n",
    "                paragraph_lower.count('opposite'),\n",
    "                paragraph_lower.count(r'\\b(or)\\b'),\n",
    "                paragraph_lower.count(r'\\b(our)\\b'),\n",
    "                paragraph_lower.count('outside'),\n",
    "                paragraph_lower.count('over'),\n",
    "                paragraph_lower.count(r'\\b(some)\\b'),\n",
    "                paragraph_lower.count('somebody'),\n",
    "                paragraph_lower.count('someone'),\n",
    "                paragraph_lower.count('something'),\n",
    "                paragraph_lower.count('such'),\n",
    "                paragraph_lower.count('than'),\n",
    "                paragraph_lower.count('that'),\n",
    "                paragraph_lower.count(r'\\b(the)\\b'),\n",
    "                paragraph_lower.count(r'\\b(their)\\b'),\n",
    "                paragraph_lower.count(r'\\b(them)\\b'),\n",
    "                paragraph_lower.count(r'\\b(these)\\b'),\n",
    "                paragraph_lower.count(r'\\b(they)\\b'),\n",
    "                paragraph_lower.count(r'\\b(this)\\b'),\n",
    "                paragraph_lower.count(r'\\b(those)\\b'),\n",
    "                paragraph_lower.count('though'),\n",
    "                paragraph_lower.count('through'),\n",
    "                paragraph_lower.count(r'\\b(till)\\b'),\n",
    "                paragraph_lower.count(r'\\b(to)\\b'),\n",
    "                paragraph_lower.count('toward '),\n",
    "                paragraph_lower.count('towards'),\n",
    "                paragraph_lower.count('under'),\n",
    "                paragraph_lower.count('unless'),\n",
    "                paragraph_lower.count('whether'),\n",
    "                paragraph_lower.count('which'),\n",
    "                paragraph_lower.count('while'),\n",
    "                paragraph_lower.count(r'\\b(who)\\b'),\n",
    "                paragraph_lower.count('whoever'),\n",
    "                paragraph_lower.count('whom'),\n",
    "                paragraph_lower.count('whose'),\n",
    "                paragraph_lower.count(r'\\b(will)\\b'),\n",
    "                paragraph_lower.count(r'\\b(with)\\b'),\n",
    "                paragraph_lower.count('within'),\n",
    "                paragraph_lower.count('without'),\n",
    "                paragraph_lower.count('worth'),\n",
    "                paragraph_lower.count('would'),\n",
    "                paragraph_lower.count(r'\\b(yes)\\b'),\n",
    "                paragraph_lower.count(r'\\b(you)\\b'),\n",
    "                paragraph_lower.count(r'\\b(your)\\b'),\n",
    "                paragraph_lower.count(r'([^\\w\\s])\\1')/(char_two_grams + epsilon),\n",
    "            ]\n",
    "            F = np.concatenate((F, F_sents))\n",
    "\n",
    "        return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_extractor = StylometryExtractor(corpus)  # Less than 1 minute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved corpus variable\n",
      "lowered\n",
      "tokenized\n",
      "pos_tagged\n",
      "general freq dist done\n",
      "vocabulary initialized\n",
      "adjectives done\n",
      "conjunctions done\n",
      "interrogatives done\n",
      "nouns done\n",
      "verbs done\n",
      "3grams done\n",
      "5grams done\n",
      "oov done\n",
      "collected freq_dists for adjectives\n",
      "collected freq_dists for conjunctions\n",
      "collected freq_dists for interrogatives\n",
      "collected freq_dists for nouns\n",
      "collected freq_dists for verbs\n",
      "collected freq_dists for oovs\n",
      "collected freq_dists for 3grams\n",
      "collected freq_dists for 5grams\n",
      "starting collecting most commons\n"
     ]
    }
   ],
   "source": [
    "my_extractor.preprocess_corpus()    # Around 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parasites…',\n",
       " 'uhh…',\n",
       " 'shui-bien',\n",
       " 'dealer/user',\n",
       " 'to/kill',\n",
       " 'anymore™️',\n",
       " 'aewacs',\n",
       " 't-129',\n",
       " 'r-77',\n",
       " 'tiktok',\n",
       " 'redditors',\n",
       " '2034.',\n",
       " 'dicked',\n",
       " 'dellusional',\n",
       " 'turtle-dove',\n",
       " 'covid',\n",
       " 'sisolak',\n",
       " 'snap/ebt',\n",
       " 'dtjs',\n",
       " 'homeland.',\n",
       " 'under-60s',\n",
       " 'justiceforjanuary6',\n",
       " 'dumbass',\n",
       " 'nobodys',\n",
       " 'ballots/what',\n",
       " 'gqp',\n",
       " 'willing/able/some',\n",
       " 'hungarian-ukrainian',\n",
       " 'clients.',\n",
       " 'ahahahha',\n",
       " 'do.',\n",
       " 'lenhs',\n",
       " 'tabloid-type',\n",
       " 'research/art',\n",
       " 'force….what',\n",
       " 'muskets…and',\n",
       " 'non-long',\n",
       " 'el-sisi',\n",
       " 'price.',\n",
       " 'study/do',\n",
       " 'explore/develop',\n",
       " 'bayraktars',\n",
       " 'kukuryki',\n",
       " 'non-du',\n",
       " '2016.',\n",
       " 'dem-friendly',\n",
       " 'is.',\n",
       " 'oush',\n",
       " 'body/money/health/',\n",
       " 'family/',\n",
       " 'post-brexit',\n",
       " \"republican'ts\",\n",
       " 'republicans/abbott',\n",
       " '..',\n",
       " '3.5t',\n",
       " '2.8t',\n",
       " \"'help\",\n",
       " 'accusing/blaming',\n",
       " 'becuz',\n",
       " 'kashoggi',\n",
       " 'bailifs',\n",
       " 'transcriptionists',\n",
       " 'easily.but',\n",
       " 'standards/bribes',\n",
       " 'complete/cheaper',\n",
       " 'standards/forged',\n",
       " 'security/falsified',\n",
       " 'materials/falsified',\n",
       " 'buildings/etc',\n",
       " 'western-backed',\n",
       " '1789.',\n",
       " 'guess……',\n",
       " 'wages…',\n",
       " 'extremistas',\n",
       " 'zelensky',\n",
       " 'non-prosecution',\n",
       " 'pre-2008',\n",
       " 'enemies-',\n",
       " 'up/',\n",
       " 'km^2',\n",
       " '141,750,000',\n",
       " 'competitive/close',\n",
       " 'phisiologyical',\n",
       " '🏥🏥🏥🏥🏥🏥',\n",
       " 'dmvphobiat',\n",
       " 'reuters.com',\n",
       " 'upvoting',\n",
       " 'fall/early',\n",
       " 'insuficient',\n",
       " 'emitting.',\n",
       " 'morals/judgements/values',\n",
       " 'parents/religion/society/',\n",
       " 'education/enpowerment',\n",
       " 'counterintel',\n",
       " 'gummint',\n",
       " 'hieronymusanonymous',\n",
       " 'markets.businessinsider.com',\n",
       " '/r/worldnews',\n",
       " 'flaired',\n",
       " 'cryptocurrencies']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_extractor.get_values('oov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.83333333e-01 3.33333333e-02 0.00000000e+00 2.00000000e-01\n",
      " 3.16666667e-01 3.68421053e-01 5.26315789e-02 2.63157895e-01\n",
      " 2.63157895e-01 5.26315789e-02 1.16666667e-01 2.76595745e-01\n",
      " 1.27659574e-01 1.27659574e-01 4.25531915e-02 3.84615385e-01\n",
      " 7.69230769e-02 3.07692308e-01 0.00000000e+00 2.30769231e-01\n",
      " 1.66666667e-01 3.33333333e-01 1.66666667e-01 3.33333333e-01\n",
      " 1.66666667e-01 1.66666667e-01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 8.33333333e-01 0.00000000e+00 5.00000000e-01\n",
      " 0.00000000e+00 5.00000000e-01 0.00000000e+00 0.00000000e+00\n",
      " 8.47457627e-02 2.20338983e-01 2.54237288e-01 1.69491525e-02\n",
      " 0.00000000e+00 0.00000000e+00 4.00000000e-01 0.00000000e+00\n",
      " 7.69230769e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 6.66666667e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 6.00000000e-01 4.70000000e-01 2.00000000e-02 7.00000000e+00\n",
      " 0.00000000e+00 1.00000000e+00 2.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 5.00000000e+00 3.00000000e+00 5.00000000e+00\n",
      " 0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 5.00000000e+00 2.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 4.00000000e+00 5.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.42857143e-01 2.85714286e-01 7.14285714e-02\n",
      " 2.85714286e-01 1.42857143e-01 0.00000000e+00 0.00000000e+00\n",
      " 7.14285714e-02 5.00000000e-01 4.28571429e+00 9.28571429e-01\n",
      " 1.40000000e+01 7.00000000e+00 2.00000000e+00 4.00000000e+00\n",
      " 1.00000000e+00 4.00000000e+00 2.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 2.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 2.00000000e+00 2.00000000e-01 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 8.33333333e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.66666667e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 7.14285714e-02\n",
      " 2.85714286e-01 2.85714286e-01 0.00000000e+00 7.14285714e-02\n",
      " 1.00000000e+10 1.42857143e-01 0.00000000e+00 0.00000000e+00\n",
      " 3.00000000e+01 7.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 7.14285714e-02 2.50000000e-01 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 7.14285714e-02 0.00000000e+00 1.42857143e-01\n",
      " 0.00000000e+00 0.00000000e+00 5.00000000e-01 0.00000000e+00\n",
      " 7.14285714e-02 7.14285714e-02 7.14285714e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 7.14285714e-02 0.00000000e+00\n",
      " 0.00000000e+00 3.57142857e-01 0.00000000e+00 7.14285714e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 7.14285714e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "f = my_extractor.stylometry_extractor(\"David screw this! I am going to go to the galapagos asap\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"stylometry_extractor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(my_extractor, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stylometry_extractor.pkl\", \"rb\") as f:\n",
    "    loaded_obj = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.83333333e-01 3.33333333e-02 0.00000000e+00 2.00000000e-01\n",
      " 3.16666667e-01 3.68421053e-01 5.26315789e-02 2.63157895e-01\n",
      " 2.63157895e-01 5.26315789e-02 1.16666667e-01 2.76595745e-01\n",
      " 1.27659574e-01 1.27659574e-01 4.25531915e-02 3.84615385e-01\n",
      " 7.69230769e-02 3.07692308e-01 0.00000000e+00 2.30769231e-01\n",
      " 1.66666667e-01 3.33333333e-01 1.66666667e-01 3.33333333e-01\n",
      " 1.66666667e-01 1.66666667e-01 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 8.33333333e-01 0.00000000e+00 5.00000000e-01\n",
      " 0.00000000e+00 5.00000000e-01 0.00000000e+00 0.00000000e+00\n",
      " 8.47457627e-02 2.20338983e-01 2.54237288e-01 1.69491525e-02\n",
      " 0.00000000e+00 0.00000000e+00 4.00000000e-01 0.00000000e+00\n",
      " 7.69230769e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 6.66666667e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 6.00000000e-01 4.70000000e-01 2.00000000e-02 7.00000000e+00\n",
      " 0.00000000e+00 1.00000000e+00 2.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 5.00000000e+00 3.00000000e+00 5.00000000e+00\n",
      " 0.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 5.00000000e+00 2.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 4.00000000e+00 5.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.42857143e-01 2.85714286e-01 7.14285714e-02\n",
      " 2.85714286e-01 1.42857143e-01 0.00000000e+00 0.00000000e+00\n",
      " 7.14285714e-02 5.00000000e-01 4.28571429e+00 9.28571429e-01\n",
      " 1.40000000e+01 7.00000000e+00 2.00000000e+00 4.00000000e+00\n",
      " 1.00000000e+00 4.00000000e+00 2.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 2.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 2.00000000e+00 2.00000000e-01 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 8.33333333e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.66666667e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 7.14285714e-02\n",
      " 2.85714286e-01 2.85714286e-01 0.00000000e+00 7.14285714e-02\n",
      " 1.00000000e+10 1.42857143e-01 0.00000000e+00 0.00000000e+00\n",
      " 3.00000000e+01 7.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 7.14285714e-02 2.50000000e-01 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 7.14285714e-02 0.00000000e+00 1.42857143e-01\n",
      " 0.00000000e+00 0.00000000e+00 5.00000000e-01 0.00000000e+00\n",
      " 7.14285714e-02 7.14285714e-02 7.14285714e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 7.14285714e-02 0.00000000e+00\n",
      " 0.00000000e+00 3.57142857e-01 0.00000000e+00 7.14285714e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 7.14285714e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Second try...\n",
    "\n",
    "f = loaded_obj.stylometry_extractor(\"David screw this! I am going to go to the galapagos asap\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
